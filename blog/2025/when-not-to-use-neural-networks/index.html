<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>When Not to Use Neural Networks | Dr. Mario A. Hevia Fajardo</title> <meta name="author" content="Mario A. Hevia Fajardo"> <meta name="description" content="While neural networks are powerful, they aren't always the best tool for the job. This post explores cases where simpler models outperform them, with Python code snippets and experiments."> <meta name="keywords" content="evolutionary-computation, evolutionary-algorithms, runtime-analysis, research, theory"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/icon.png"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://mariohevia.github.io/blog/2025/when-not-to-use-neural-networks/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">{
      "title": "When Not to Use Neural Networks",
      "description": "While neural networks are powerful, they aren't always the best tool for the job. This post explores cases where simpler models outperform them, with Python code snippets and experiments.",
      "published": "February 15, 2025",
      "authors": [
        {
          "author": "Mario A. Hevia Fajardo",
          "authorURL": "https://mhevia.com",
          "affiliations": [
            {
              "name": "University of Birmingham",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Dr. Mario A. Hevia Fajardo</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right font-weight-bold" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">Home</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>When Not to Use Neural Networks</h1> <p>While neural networks are powerful, they aren't always the best tool for the job. This post explores cases where simpler models outperform them, with Python code snippets and experiments.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#introduction">Introduction</a></div> <div><a href="#when-simplicity-wins">When Simplicity Wins</a></div> <div><a href="#example-1-small-datasets">Example 1: Small Datasets</a></div> <div><a href="#example-2-high-dimensional-data">Example 2: High-dimensional data</a></div> <div><a href="#other-things-to-consider-with-nns">Other things to consider with NNs</a></div> <div><a href="#conclusions">Conclusions</a></div> </nav> </d-contents> <p>Neural networks (NNs) are a hot topic nowadays, mainly because they are one of the best options for computer vision, natural language processing, reinforcement learning, and, more recently, with new LLMs coming out every other day. However, in many cases, it is better to use a different ML algorithm. In this post, I’ll share a few examples where NNs are not the best choice, highlighting situations where other ML models can deliver better results.</p> <h3 id="when-simplicity-wins">When Simplicity Wins</h3> <p>Neural networks are good at modelling complex, non-linear relationships. However, this strength can turn into a weakness when:</p> <ul> <li>The dataset is small, which makes NNs more likely to overfit.</li> <li>The data is high-dimensional, with many non-informative features, and the NNs have to learn which features are relevant, which is difficult.</li> <li>Interpretability is important, as NNs are often considered black boxes.</li> </ul> <p>Let’s go through some experiments to show these scenarios.</p> <h3 id="example-1-small-datasets">Example 1: Small Datasets</h3> <p>On small datasets, NNs can easily overfit to the training data. Other algorithms may capture the underlying patterns more effectively, allowing them to generalise better.</p> <p>In the example below, I compare a neural network with a Gaussian Process classifier (GP) and a Support Vector Machine (SVM). To emphasise the difference, I limit the training set to just 50 data points. The tests are repeated 100 times to ensure that what we are seeing is not just luck.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">scores_dict</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">"</span><span class="s">SVM</span><span class="sh">"</span><span class="p">:[],</span>
    <span class="sh">"</span><span class="s">Gaussian Process</span><span class="sh">"</span><span class="p">:[],</span>
    <span class="sh">"</span><span class="s">Neural Network</span><span class="sh">"</span><span class="p">:[]</span>
<span class="p">}</span>
<span class="k">for</span> <span class="n">seed</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="c1"># Synthetic dataset
</span>    <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="nf">make_moons</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span> <span class="p">,</span><span class="n">noise</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="nf">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
    
    <span class="c1"># Classifiers to compare
</span>    <span class="n">svm</span> <span class="o">=</span> <span class="nc">SVC</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">gp</span> <span class="o">=</span> <span class="nc">GaussianProcessClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">nn</span> <span class="o">=</span> <span class="nc">MLPClassifier</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
    
    <span class="n">classifiers</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">(</span><span class="sh">"</span><span class="s">SVM</span><span class="sh">"</span><span class="p">,</span> <span class="n">svm</span><span class="p">),</span>
        <span class="p">(</span><span class="sh">"</span><span class="s">Gaussian Process</span><span class="sh">"</span><span class="p">,</span> <span class="n">gp</span><span class="p">),</span>
        <span class="p">(</span><span class="sh">"</span><span class="s">Neural Network</span><span class="sh">"</span><span class="p">,</span><span class="n">nn</span><span class="p">)]</span>

    <span class="c1"># Fitting and scoring the classifiers
</span>    <span class="n">last_models</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">clf</span> <span class="ow">in</span> <span class="n">classifiers</span><span class="p">:</span>
        <span class="n">clf</span> <span class="o">=</span> <span class="nf">make_pipeline</span><span class="p">(</span><span class="nc">StandardScaler</span><span class="p">(),</span> <span class="n">clf</span><span class="p">)</span>
        <span class="n">clf</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
        <span class="n">score</span> <span class="o">=</span> <span class="n">clf</span><span class="p">.</span><span class="nf">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
        <span class="n">scores_dict</span><span class="p">[</span><span class="n">name</span><span class="p">].</span><span class="nf">append</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
        <span class="n">last_models</span><span class="p">.</span><span class="nf">append</span><span class="p">((</span><span class="n">name</span><span class="p">,</span><span class="n">clf</span><span class="p">))</span>
<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">scores</span> <span class="ow">in</span> <span class="n">scores_dict</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="nf">sum</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span><span class="o">/</span><span class="nf">len</span><span class="p">(</span><span class="n">scores</span><span class="p">))</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>SVM
0.688778947368421
Gaussian Process
0.6964105263157895
Neural Network
0.6592210526315789
</code></pre></div></div> <p>From the results, we can see that, on average, it is better to use the SVM or GP. The reason becomes clearer when we plot the decision boundary of each algorithm against the data.</p> <p style="text-align:center"><img src="../../../assets/img/blog_images/2025-02-15-when-not-to-use-neural-networks/small_data.png" alt="Comparison with small data"></p> <p>The plots show that the neural network (right) is overfitting, while the other models (SVM on the left, GP in the centre) generalise better to the overall distribution.</p> <h3 id="example-2-high-dimensional-data">Example 2: High-dimensional data</h3> <p>Another place where NNs often struggle is with high-dimensional structured data that has only a small proportion of informative features. This is challenging because NNs rely on large amounts of data to learn meaningful patterns, and when most features are irrelevant, they can easily learn noise instead. Unlike models like decision trees or Lasso regression, which can automatically ignore unimportant features, NNs tend to spread weight across all inputs, making them less efficient in such cases.</p> <p>In the following example, I compare XGBoost, a tree-based algorithm, against a neural network. As before, the tests are run 100 times with different random seeds to ensure reliable results.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">scores_dict</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">"</span><span class="s">XGBoost</span><span class="sh">"</span><span class="p">:[],</span>
    <span class="sh">"</span><span class="s">Neural Network</span><span class="sh">"</span><span class="p">:[]</span>
<span class="p">}</span>
<span class="k">for</span> <span class="n">seed</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="c1"># Synthetic dataset
</span>    <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="nf">make_classification</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">n_informative</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">n_redundant</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="nf">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>

    <span class="c1"># Classifiers to compare
</span>    <span class="n">nn</span> <span class="o">=</span> <span class="nc">MLPClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">xgboost</span> <span class="o">=</span> <span class="n">xgb</span><span class="p">.</span><span class="nc">XGBClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>

    <span class="n">classifiers</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">(</span><span class="sh">"</span><span class="s">XGBoost</span><span class="sh">"</span><span class="p">,</span> <span class="n">xgboost</span><span class="p">),</span>
        <span class="p">(</span><span class="sh">"</span><span class="s">Neural Network</span><span class="sh">"</span><span class="p">,</span><span class="n">nn</span><span class="p">)]</span>

    <span class="c1"># Fitting and scoring the classifiers
</span>    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">clf</span> <span class="ow">in</span> <span class="n">classifiers</span><span class="p">:</span>
        <span class="n">clf</span> <span class="o">=</span> <span class="nf">make_pipeline</span><span class="p">(</span><span class="nc">StandardScaler</span><span class="p">(),</span> <span class="n">clf</span><span class="p">)</span>
        <span class="n">clf</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
        <span class="n">score</span> <span class="o">=</span> <span class="n">clf</span><span class="p">.</span><span class="nf">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
        <span class="n">scores_dict</span><span class="p">[</span><span class="n">name</span><span class="p">].</span><span class="nf">append</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">scores</span> <span class="ow">in</span> <span class="n">scores_dict</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="nf">sum</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span><span class="o">/</span><span class="nf">len</span><span class="p">(</span><span class="n">scores</span><span class="p">))</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>XGBoost
0.8647499999999999
Neural Network
0.76325
</code></pre></div></div> <p>With 200 features but only 15 informative, the neural network has to learn which features are relevant, which is difficult unless regularization (L1/L2, dropout, batch norm) is applied effectively. On the other hand, XGBoost automatically identifies the most relevant features (15 informative features) while ignoring the irrelevant 185 noisy ones.</p> <h3 id="other-things-to-consider-with-nns">Other things to consider with NNs</h3> <p><strong>Interpretability Needs:</strong> In many applications, it is not sufficient to just predict the next label. Often, it is essential to provide the decision maker with the reasoning behind the decision. When using neural networks, it is much harder to interpret and understand this reasoning, while other models, such as decision trees or linear models, offer more transparent and easily interpretable decision-making processes.</p> <p><strong>Initialisation sensitivity:</strong> One of the strengths of neural networks is their non-linearity, which enables them to model complex scenarios. However, this also leads to a non-convex loss function with multiple local minima. As a result, different random weight initializations can cause the network to converge to different local minima, potentially affecting the model’s performance.</p> <p><strong>Many hyperparameters:</strong> Neural networks require tuning a number of hyperparameters, such as the number of hidden neurons, layers, learning rate, and the number of training iterations. Finding the optimal combination of these parameters is crucial for achieving good performance, but it can be time-consuming and computationally expensive.</p> <h3 id="conclusions">Conclusions</h3> <p>While neural networks are powerful, they are not always the best choice. Simpler models can outperform them in terms of accuracy, efficiency, and interpretability, especially with small datasets, tabular data, or when transparency is crucial. Selecting the right model requires a deep understanding of both the data and the problem at hand.</p> <h5 id="acknowledgements"><span style="opacity: 0.4;"><small>Acknowledgements</small></span></h5> <p><span style="opacity: 0.4;"><small>I would like to thank my friend <a href="https://www.linkedin.com/in/aldo-segura/" rel="external nofollow noopener" target="_blank">Aldo Encarnacion</a> and my wife <a href="https://www.linkedin.com/in/jagoda-karpowicz/" rel="external nofollow noopener" target="_blank">Jagoda Hevia Karpowicz</a>, as it was through our discussions that the idea for this blog post was born.</small></span></p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Mario A. Hevia Fajardo. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: May 06, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>