<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Predicting Bike Sharing Demand: A Step-by-Step Guide to Building a Regression Model | Dr. Mario Hevia Fajardo</title> <meta name="author" content="Mario A. Hevia Fajardo"> <meta name="description" content="Using 2 years of weather and bike-sharing data, I built a machine learning model to predict the number of bike rentals in any given hour. This post covers the exploratory data analysis, data preprocessing, feature selection, and model evaluation."> <meta name="keywords" content="evolutionary-computation, evolutionary-algorithms, runtime-analysis, research, theory"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/icon.png"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://mariohevia.github.io/blog/2025/weather-data-to-bike-sharing-demand/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">{
      "title": "Predicting Bike Sharing Demand: A Step-by-Step Guide to Building a Regression Model",
      "description": "Using 2 years of weather and bike-sharing data, I built a machine learning model to predict the number of bike rentals in any given hour. This post covers the exploratory data analysis, data preprocessing, feature selection, and model evaluation.",
      "published": "February 24, 2025",
      "authors": [
        {
          "author": "Mario A. Hevia Fajardo",
          "authorURL": "https://mhevia.com",
          "affiliations": [
            {
              "name": "University of Birmingham",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Dr. Mario Hevia Fajardo</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right font-weight-bold" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">Home</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Predicting Bike Sharing Demand: A Step-by-Step Guide to Building a Regression Model</h1> <p>Using 2 years of weather and bike-sharing data, I built a machine learning model to predict the number of bike rentals in any given hour. This post covers the exploratory data analysis, data preprocessing, feature selection, and model evaluation.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#data-preparation">Data Preparation</a></div> <div><a href="#data-splitting">Data Splitting</a></div> <div><a href="#feature-engineering">Feature Engineering</a></div> <div><a href="#model-selection">Model Selection</a></div> <div><a href="#model-training">Model Training</a></div> <div><a href="#model-evaluation">Model Evaluation</a></div> <div><a href="#model-interpretation">Model Interpretation</a></div> <div><a href="#other-things-to-consider">Other things to consider</a></div> <div><a href="#conclusions">Conclusions</a></div> </nav> </d-contents> <p>This project began as a short exercise for a job interview, but I wanted to expand on it in a more instructional way, explaining my thought process, the reasoning behind certain decisions, and how these steps can be applied to any classification or regression problem. Since the original dataset was provided as part of the interview, I’ll be using an open-source dataset instead for this blog post.</p> <p>As you can guess from the title, the dataset contains bike-sharing demand data. More specifically, it records the hourly count of rental bikes between 2011 and 2012 in the Capital Bikeshare system, along with corresponding weather and seasonal information. This dataset was introduced in the paper <a href="https://link.springer.com/article/10.1007/s13748-013-0040-3" rel="external nofollow noopener" target="_blank">Event labeling combining ensemble detectors and background knowledge</a> by Hadi Fanaee-T and João Gama in 2013.</p> <p>I consider it a great dataset for guiding you through the steps needed to analyse a dataset and build a regression model. If you want to follow along the blog you can download the <a href="../../../assets/img/blog_images/2025-02-24-weather-data-to-bike-sharing-demand/bike-sharing.ipynb">Jupyter notebook</a>.</p> <h3 id="data-preparation">Data Preparation</h3> <p>Let’s begin by downloading the dataset and preparing the data. The dataset is hosted on Kaggle <a href="https://www.kaggle.com/datasets/lakshmi25npathi/bike-sharing-dataset/data" rel="external nofollow noopener" target="_blank">here</a>. To download it, you’ll first need to install the Kaggle API on your computer, generate an API key, and then run the following command in a terminal.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kaggle datasets download -d lakshmi25npathi/bike-sharing-dataset
</code></pre></div></div> <p>Now, moving to Python, we can extract the dataset into a folder called <code class="language-plaintext highlighter-rouge">bike-sharing-dataset</code> and later load it into a Pandas DataFrame using:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">zipfile</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="k">with</span> <span class="n">zipfile</span><span class="p">.</span><span class="nc">ZipFile</span><span class="p">(</span><span class="sh">"</span><span class="s">bike-sharing-dataset.zip</span><span class="sh">"</span><span class="p">,</span> <span class="sh">'</span><span class="s">r</span><span class="sh">'</span><span class="p">)</span> <span class="k">as</span> <span class="n">zip_ref</span><span class="p">:</span>
    <span class="n">zip_ref</span><span class="p">.</span><span class="nf">extractall</span><span class="p">(</span><span class="sh">"</span><span class="s">bike-sharing-dataset</span><span class="sh">"</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">"</span><span class="s">./bike-sharing-dataset/hour.csv</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p>From the dataset information on <a href="https://www.kaggle.com/datasets/lakshmi25npathi/bike-sharing-dataset/data" rel="external nofollow noopener" target="_blank">Kaggle</a>, we can see that it includes three different counts we might want to predict: the number of casual users, registered users, or the total count.</p> <p>Now, we can inspect the data, check for missing values, and identify any inconsistencies across all variables.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">display</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="nf">info</span><span class="p">())</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>RangeIndex: 17379 entries, 0 to 17378
Data columns (total 17 columns):
 #   Column      Non-Null Count  Dtype  
---  ------      --------------  -----  
 0   instant     17379 non-null  int64  
 1   dteday      17379 non-null  object 
 2   season      17379 non-null  int64  
 3   yr          17379 non-null  int64  
 4   mnth        17379 non-null  int64  
 5   hr          17379 non-null  int64  
 6   holiday     17379 non-null  int64  
 7   weekday     17379 non-null  int64  
 8   workingday  17379 non-null  int64  
 9   weathersit  17379 non-null  int64  
 10  temp        17379 non-null  float64
 11  atemp       17379 non-null  float64
 12  hum         17379 non-null  float64
 13  windspeed   17379 non-null  float64
 14  casual      17379 non-null  int64  
 15  registered  17379 non-null  int64  
 16  cnt         17379 non-null  int64  
dtypes: float64(4), int64(12), object(1)
memory usage: 2.3+ MB
</code></pre></div></div> <p>The first thing to notice is that the <code class="language-plaintext highlighter-rouge">dteday</code> column, which represents a date, is not automatically imported as a <code class="language-plaintext highlighter-rouge">datetime</code> type by Pandas. This isn’t particularly important in this dataset since other features already encode the date, but we’ll address it during the exploratory data analysis.</p> <p>Another key point is that there are no missing values, so we don’t need to apply imputation techniques or remove rows/columns due to excessive missing data.</p> <p>Now, we can examine the distribution of values in each column using <code class="language-python">df.describe()</code>.</p> <div style="overflow-x: auto;"> <style scoped="">.dataframe tbody tr th:only-of-type{vertical-align:middle}.dataframe tbody tr th{vertical-align:top}.dataframe thead th{text-align:right}</style> <table border="1" class="dataframe"> <thead> <tr style="text-align: right;"> <th></th> <th>instant</th> <th>season</th> <th>yr</th> <th>mnth</th> <th>hr</th> <th>holiday</th> <th>weekday</th> <th>workingday</th> <th>weathersit</th> <th>temp</th> <th>atemp</th> <th>hum</th> <th>windspeed</th> <th>casual</th> <th>registered</th> <th>cnt</th> </tr> </thead> <tbody> <tr> <th>count</th> <td>17379.0000</td> <td>17379.000000</td> <td>17379.000000</td> <td>17379.000000</td> <td>17379.000000</td> <td>17379.000000</td> <td>17379.000000</td> <td>17379.000000</td> <td>17379.000000</td> <td>17379.000000</td> <td>17379.000000</td> <td>17379.000000</td> <td>17379.000000</td> <td>17379.000000</td> <td>17379.000000</td> <td>17379.000000</td> </tr> <tr> <th>mean</th> <td>8690.0000</td> <td>2.501640</td> <td>0.502561</td> <td>6.537775</td> <td>11.546752</td> <td>0.028770</td> <td>3.003683</td> <td>0.682721</td> <td>1.425283</td> <td>0.496987</td> <td>0.475775</td> <td>0.627229</td> <td>0.190098</td> <td>35.676218</td> <td>153.786869</td> <td>189.463088</td> </tr> <tr> <th>std</th> <td>5017.0295</td> <td>1.106918</td> <td>0.500008</td> <td>3.438776</td> <td>6.914405</td> <td>0.167165</td> <td>2.005771</td> <td>0.465431</td> <td>0.639357</td> <td>0.192556</td> <td>0.171850</td> <td>0.192930</td> <td>0.122340</td> <td>49.305030</td> <td>151.357286</td> <td>181.387599</td> </tr> <tr> <th>min</th> <td>1.0000</td> <td>1.000000</td> <td>0.000000</td> <td>1.000000</td> <td>0.000000</td> <td>0.000000</td> <td>0.000000</td> <td>0.000000</td> <td>1.000000</td> <td>0.020000</td> <td>0.000000</td> <td>0.000000</td> <td>0.000000</td> <td>0.000000</td> <td>0.000000</td> <td>1.000000</td> </tr> <tr> <th>25%</th> <td>4345.5000</td> <td>2.000000</td> <td>0.000000</td> <td>4.000000</td> <td>6.000000</td> <td>0.000000</td> <td>1.000000</td> <td>0.000000</td> <td>1.000000</td> <td>0.340000</td> <td>0.333300</td> <td>0.480000</td> <td>0.104500</td> <td>4.000000</td> <td>34.000000</td> <td>40.000000</td> </tr> <tr> <th>50%</th> <td>8690.0000</td> <td>3.000000</td> <td>1.000000</td> <td>7.000000</td> <td>12.000000</td> <td>0.000000</td> <td>3.000000</td> <td>1.000000</td> <td>1.000000</td> <td>0.500000</td> <td>0.484800</td> <td>0.630000</td> <td>0.194000</td> <td>17.000000</td> <td>115.000000</td> <td>142.000000</td> </tr> <tr> <th>75%</th> <td>13034.5000</td> <td>3.000000</td> <td>1.000000</td> <td>10.000000</td> <td>18.000000</td> <td>0.000000</td> <td>5.000000</td> <td>1.000000</td> <td>2.000000</td> <td>0.660000</td> <td>0.621200</td> <td>0.780000</td> <td>0.253700</td> <td>48.000000</td> <td>220.000000</td> <td>281.000000</td> </tr> <tr> <th>max</th> <td>17379.0000</td> <td>4.000000</td> <td>1.000000</td> <td>12.000000</td> <td>23.000000</td> <td>1.000000</td> <td>6.000000</td> <td>1.000000</td> <td>4.000000</td> <td>1.000000</td> <td>1.000000</td> <td>1.000000</td> <td>0.850700</td> <td>367.000000</td> <td>886.000000</td> <td>977.000000</td> </tr> </tbody> </table> </div> <p>From this, we can see that the features in the dataset are already preprocessed. The columns <code class="language-plaintext highlighter-rouge">temp</code>, <code class="language-plaintext highlighter-rouge">atemp</code>, <code class="language-plaintext highlighter-rouge">hum</code>, and <code class="language-plaintext highlighter-rouge">windspeed</code> are scaled between 0 and 1, while all other columns are ordinally encoded. This preprocessing will be beneficial during feature engineering.</p> <h3 id="data-splitting">Data Splitting</h3> <p>Since the data is time-based, we’ll use the first year and a half for training and the last six months for testing. This approach mimics a real-world scenario where historical data is used to predict future outcomes.</p> <p>Additionally, we use the <code class="language-plaintext highlighter-rouge">copy()</code> method to create a deep copy of the original DataFrame, preventing unintended modifications that could affect the original data.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_df</span> <span class="o">=</span> <span class="n">df</span><span class="p">[(</span><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">yr</span><span class="sh">'</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="o">|</span> <span class="p">((</span><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">yr</span><span class="sh">'</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">mnth</span><span class="sh">'</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="mi">6</span><span class="p">))].</span><span class="nf">copy</span><span class="p">()</span>
<span class="n">test_df</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="o">~</span><span class="n">df</span><span class="p">.</span><span class="n">index</span><span class="p">.</span><span class="nf">isin</span><span class="p">(</span><span class="n">train_df</span><span class="p">.</span><span class="n">index</span><span class="p">)].</span><span class="nf">copy</span><span class="p">()</span>
</code></pre></div></div> <p>As mentioned earlier, the dataset has three possible target variables. For this blog post, we will focus solely on predicting the total number of users. Therefore, we drop the <code class="language-plaintext highlighter-rouge">casual</code> and <code class="language-plaintext highlighter-rouge">registered</code> columns and set <code class="language-plaintext highlighter-rouge">cnt</code> as our label.</p> <p>Once again, note the use of <code class="language-plaintext highlighter-rouge">copy()</code> to ensure we create a deep copy and avoid unintended modifications to the original DataFrame.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">train_df</span><span class="p">.</span><span class="nf">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">cnt</span><span class="sh">"</span><span class="p">,</span><span class="sh">"</span><span class="s">casual</span><span class="sh">"</span><span class="p">,</span><span class="sh">"</span><span class="s">registered</span><span class="sh">"</span><span class="p">]).</span><span class="nf">copy</span><span class="p">(),</span> <span class="n">train_df</span><span class="p">[</span><span class="sh">"</span><span class="s">cnt</span><span class="sh">"</span><span class="p">].</span><span class="nf">copy</span><span class="p">()</span>
<span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">test_df</span><span class="p">.</span><span class="nf">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">cnt</span><span class="sh">"</span><span class="p">,</span><span class="sh">"</span><span class="s">casual</span><span class="sh">"</span><span class="p">,</span><span class="sh">"</span><span class="s">registered</span><span class="sh">"</span><span class="p">]).</span><span class="nf">copy</span><span class="p">(),</span> <span class="n">test_df</span><span class="p">[</span><span class="sh">"</span><span class="s">cnt</span><span class="sh">"</span><span class="p">].</span><span class="nf">copy</span><span class="p">()</span>
</code></pre></div></div> <h3 id="exploratory-data-analysis">Exploratory Data Analysis</h3> <p>The first step I like to take in an exploratory data analysis before training a model is examining the correlation matrix; a table that displays the correlation coefficients between variables, indicating the strength and direction of their relationships.</p> <p>Here, we plot a heatmap of the correlation matrix. From this, we can already see that the most useful features are likely to be <code class="language-plaintext highlighter-rouge">temp</code>, <code class="language-plaintext highlighter-rouge">atemp</code>, <code class="language-plaintext highlighter-rouge">hr</code>, and <code class="language-plaintext highlighter-rouge">hum</code>, as they all show a correlation with the target variable.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sns</span><span class="p">.</span><span class="nf">heatmap</span><span class="p">(</span><span class="n">train_df</span><span class="p">.</span><span class="nf">corr</span><span class="p">(</span><span class="n">numeric_only</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="sh">"</span><span class="s">YlGnBu</span><span class="sh">"</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div> <p style="text-align:center"><img src="../../../assets/img/blog_images/2025-02-24-weather-data-to-bike-sharing-demand/output_22_1.png" alt="Correlation matrix" style="max-width: 100%; height: auto;"></p> <p><strong>Note:</strong> We are using only the training data in this analysis to ensure that our decisions are based solely on the training set, preventing any unintended “leakage” into the final testing phase. If you are conducting an exploratory data analysis without planning to train a model later, this step would not be necessary.</p> <p>Since the data represents bike rentals, we can plot all variables against the date to identify any patterns or seasonal trends throughout the year.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_df</span><span class="p">[</span><span class="sh">'</span><span class="s">date</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">to_datetime</span><span class="p">(</span><span class="n">train_df</span><span class="p">[</span><span class="sh">'</span><span class="s">dteday</span><span class="sh">'</span><span class="p">])</span>
<span class="n">train_df</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">subplots</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="sh">"</span><span class="s">date</span><span class="sh">"</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
</code></pre></div></div> <p style="text-align:center"><img src="../../../assets/img/blog_images/2025-02-24-weather-data-to-bike-sharing-demand/output_24_0.png" alt="Correlation matrix" style="max-width: 100%; height: auto;"></p> <p>We can observe some seasonality in the number of users, which is most likely influenced by the temperature.</p> <p>Another expected factor is that the time of day affects the number of bike rentals, so let’s plot that as well.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sns</span><span class="p">.</span><span class="nf">boxplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">train_df</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="sh">'</span><span class="s">hr</span><span class="sh">'</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="sh">'</span><span class="s">cnt</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <p style="text-align:center"><img src="../../../assets/img/blog_images/2025-02-24-weather-data-to-bike-sharing-demand/output_26_1.png" alt="Correlation matrix" style="max-width: 100%; height: auto;"></p> <p>We can observe peaks in usage during commuting hours (7-8 AM and 5-7 PM), with many outliers during other hours, which could potentially affect the performance of the models.</p> <p>Another interesting aspect to explore is the daily change in bike rentals.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">daily_data</span> <span class="o">=</span> <span class="n">train_df</span><span class="p">.</span><span class="nf">groupby</span><span class="p">(</span><span class="sh">'</span><span class="s">date</span><span class="sh">'</span><span class="p">).</span><span class="nf">agg</span><span class="p">({</span><span class="sh">'</span><span class="s">cnt</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">sum</span><span class="sh">'</span><span class="p">}).</span><span class="nf">reset_index</span><span class="p">()</span>
<span class="n">daily_data</span><span class="p">[</span><span class="sh">'</span><span class="s">change</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">daily_data</span><span class="p">[</span><span class="sh">'</span><span class="s">cnt</span><span class="sh">'</span><span class="p">].</span><span class="nf">diff</span><span class="p">()</span>
<span class="n">daily_data</span><span class="p">[</span><span class="sh">'</span><span class="s">change</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">daily_data</span><span class="p">[</span><span class="sh">'</span><span class="s">change</span><span class="sh">'</span><span class="p">].</span><span class="nf">fillna</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">threshold</span> <span class="o">=</span> <span class="n">daily_data</span><span class="p">[</span><span class="sh">'</span><span class="s">change</span><span class="sh">'</span><span class="p">].</span><span class="nf">std</span><span class="p">()</span> <span class="o">*</span> <span class="mi">2</span>  <span class="c1"># Threshold: 2 * standard deviation
</span><span class="n">anomalies</span> <span class="o">=</span> <span class="n">daily_data</span><span class="p">[</span><span class="nf">abs</span><span class="p">(</span><span class="n">daily_data</span><span class="p">[</span><span class="sh">'</span><span class="s">change</span><span class="sh">'</span><span class="p">])</span> <span class="o">&gt;</span> <span class="n">threshold</span><span class="p">]</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="n">sns</span><span class="p">.</span><span class="nf">lineplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">daily_data</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="sh">'</span><span class="s">date</span><span class="sh">'</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="sh">'</span><span class="s">change</span><span class="sh">'</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Daily Change</span><span class="sh">'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">green</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">anomalies</span><span class="p">[</span><span class="sh">'</span><span class="s">date</span><span class="sh">'</span><span class="p">],</span> <span class="n">anomalies</span><span class="p">[</span><span class="sh">'</span><span class="s">change</span><span class="sh">'</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">red</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Anomaly</span><span class="sh">'</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">axhline</span><span class="p">(</span><span class="n">threshold</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">red</span><span class="sh">'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">'</span><span class="s">--</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Positive Threshold</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">axhline</span><span class="p">(</span><span class="o">-</span><span class="n">threshold</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">red</span><span class="sh">'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">'</span><span class="s">--</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Negative Threshold</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <p style="text-align:center"><img src="../../../assets/img/blog_images/2025-02-24-weather-data-to-bike-sharing-demand/output_28_1.png" alt="Correlation matrix" style="max-width: 100%; height: auto;"></p> <p>This suggests that we could potentially use data from the previous day to improve predictions for the following day. Let’s explore that possibility.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_df</span><span class="p">[</span><span class="sh">'</span><span class="s">change</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">train_df</span><span class="p">[</span><span class="sh">'</span><span class="s">cnt</span><span class="sh">'</span><span class="p">]</span> <span class="o">-</span> <span class="n">train_df</span><span class="p">[</span><span class="sh">'</span><span class="s">cnt</span><span class="sh">'</span><span class="p">].</span><span class="nf">shift</span><span class="p">(</span><span class="mi">24</span><span class="p">)</span>
<span class="n">train_df</span><span class="p">[</span><span class="sh">'</span><span class="s">change</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">train_df</span><span class="p">[</span><span class="sh">'</span><span class="s">change</span><span class="sh">'</span><span class="p">].</span><span class="nf">fillna</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">threshold</span> <span class="o">=</span> <span class="n">train_df</span><span class="p">[</span><span class="sh">'</span><span class="s">change</span><span class="sh">'</span><span class="p">].</span><span class="nf">std</span><span class="p">()</span> <span class="o">*</span> <span class="mi">2</span>  <span class="c1"># Threshold: 2 * standard deviation
</span><span class="n">anomalies</span> <span class="o">=</span> <span class="n">train_df</span><span class="p">[</span><span class="nf">abs</span><span class="p">(</span><span class="n">train_df</span><span class="p">[</span><span class="sh">'</span><span class="s">change</span><span class="sh">'</span><span class="p">])</span> <span class="o">&gt;</span> <span class="n">threshold</span><span class="p">]</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="n">sns</span><span class="p">.</span><span class="nf">lineplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">train_df</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="sh">'</span><span class="s">date</span><span class="sh">'</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="sh">'</span><span class="s">change</span><span class="sh">'</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Daily Change</span><span class="sh">'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">green</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">anomalies</span><span class="p">[</span><span class="sh">'</span><span class="s">date</span><span class="sh">'</span><span class="p">],</span> <span class="n">anomalies</span><span class="p">[</span><span class="sh">'</span><span class="s">change</span><span class="sh">'</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">red</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Anomaly</span><span class="sh">'</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">axhline</span><span class="p">(</span><span class="n">threshold</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">red</span><span class="sh">'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">'</span><span class="s">--</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Positive Threshold</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">axhline</span><span class="p">(</span><span class="o">-</span><span class="n">threshold</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">red</span><span class="sh">'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">'</span><span class="s">--</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Negative Threshold</span><span class="sh">'</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Total number of data points:</span><span class="sh">"</span><span class="p">,</span> <span class="n">train_df</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Number of anomalies:</span><span class="sh">"</span><span class="p">,</span> <span class="n">anomalies</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Percentage of anomalies: </span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="n">anomalies</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">/</span><span class="n">train_df</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">%</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Total number of data points: 13003
Number of anomalies: 860
Percentage of anomalies: 6.61%
</code></pre></div></div> <p style="text-align:center"><img src="../../../assets/img/blog_images/2025-02-24-weather-data-to-bike-sharing-demand/output_30_1.png" alt="Correlation matrix" style="max-width: 100%; height: auto;"></p> <p>From this plot, we can see that there are significantly more outliers compared to the daily count, though the percentage of anomalies remains relatively small. However, if we replot the correlation matrix, we can observe that these engineered feature could still provide valuable insights.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_df</span><span class="p">[</span><span class="sh">'</span><span class="s">cnt_prev_day</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">train_df</span><span class="p">[</span><span class="sh">'</span><span class="s">cnt</span><span class="sh">'</span><span class="p">].</span><span class="nf">shift</span><span class="p">(</span><span class="mi">24</span><span class="p">)</span>
<span class="n">train_df</span><span class="p">[</span><span class="sh">'</span><span class="s">cnt_prev_day</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">train_df</span><span class="p">[</span><span class="sh">'</span><span class="s">cnt_prev_day</span><span class="sh">'</span><span class="p">].</span><span class="nf">fillna</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">9</span><span class="p">))</span>
<span class="n">sns</span><span class="p">.</span><span class="nf">heatmap</span><span class="p">(</span><span class="n">train_df</span><span class="p">.</span><span class="nf">corr</span><span class="p">(</span><span class="n">numeric_only</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="sh">"</span><span class="s">YlGnBu</span><span class="sh">"</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div> <p style="text-align:center"><img src="../../../assets/img/blog_images/2025-02-24-weather-data-to-bike-sharing-demand/output_32_1.png" alt="Correlation matrix" style="max-width: 100%; height: auto;"></p> <h3 id="feature-engineering">Feature Engineering</h3> <p>From the data analysis, we observed that the count from the previous day could be useful, so we will add that first.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X_train_transformed</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span>
<span class="n">X_train_transformed</span><span class="p">[</span><span class="sh">'</span><span class="s">cnt_prev_day</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">.</span><span class="nf">shift</span><span class="p">(</span><span class="mi">24</span><span class="p">)</span>
<span class="n">X_train_transformed</span><span class="p">[</span><span class="sh">'</span><span class="s">cnt_prev_day</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">X_train_transformed</span><span class="p">[</span><span class="sh">'</span><span class="s">cnt_prev_day</span><span class="sh">'</span><span class="p">].</span><span class="nf">fillna</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></div> <p>As mentioned earlier, the features in this dataset have already undergone some preprocessing. For instance, <code class="language-plaintext highlighter-rouge">temp</code>, <code class="language-plaintext highlighter-rouge">atemp</code>, <code class="language-plaintext highlighter-rouge">hum</code>, and <code class="language-plaintext highlighter-rouge">windspeed</code> are scaled between 0 and 1, the remaining features are ordinally encoded, and there are no missing values. The only modifications we might consider are using one-hot encoding instead of ordinal encoding, or testing a decomposition technique like Principal Component Analysis (PCA). I’ve created three DataFrames to evaluate which encoding method works best and whether PCA proves useful. I won’t show the code here, as it’s a bit more involved, but you can find it in the <a href="../../../assets/img/blog_images/2025-02-24-weather-data-to-bike-sharing-demand/bike-sharing.ipynb">Jupyter notebook</a>.</p> <h3 id="model-selection">Model Selection</h3> <p>Here, we test several algorithms with different parameter settings using grid search. The grid search explores a range of parameter values, though the selected parameters are not highly optimised.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">models</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">"</span><span class="s">Random Forest</span><span class="sh">"</span><span class="p">:</span> <span class="nc">RandomForestRegressor</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>
    <span class="sh">"</span><span class="s">Hist Gradient Boosting</span><span class="sh">"</span><span class="p">:</span> <span class="nc">HistGradientBoostingRegressor</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">early_stopping</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
    <span class="sh">'</span><span class="s">ElasticNet</span><span class="sh">'</span><span class="p">:</span> <span class="nc">ElasticNet</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>
    <span class="sh">'</span><span class="s">xgb</span><span class="sh">'</span><span class="p">:</span> <span class="n">xgb</span><span class="p">.</span><span class="nc">XGBRegressor</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="p">}</span>

<span class="n">param_grids</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">"</span><span class="s">Random Forest</span><span class="sh">"</span><span class="p">:</span> <span class="p">{</span><span class="sh">"</span><span class="s">n_estimators</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">]},</span>
    <span class="sh">"</span><span class="s">Hist Gradient Boosting</span><span class="sh">"</span><span class="p">:</span> <span class="p">{</span><span class="sh">"</span><span class="s">max_iter</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">300</span><span class="p">]},</span>
    <span class="sh">'</span><span class="s">ElasticNet</span><span class="sh">'</span><span class="p">:</span> <span class="p">{</span><span class="sh">"</span><span class="s">alpha</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="sh">"</span><span class="s">l1_ratio</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]},</span> <span class="c1"># l1_ratio == 1.0 is equivalent to Lasso
</span>    <span class="sh">'</span><span class="s">xgb</span><span class="sh">'</span><span class="p">:{</span><span class="sh">"</span><span class="s">n_estimators</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">]}</span>
<span class="p">}</span>

<span class="n">cv</span> <span class="o">=</span> <span class="nc">KFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="k">for</span> <span class="n">X_transformed</span> <span class="ow">in</span> <span class="p">[</span><span class="n">X_transformed_1</span><span class="p">,</span><span class="n">X_transformed_2</span><span class="p">,</span><span class="n">X_transformed_3</span><span class="p">]:</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">============</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">models</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
        <span class="n">grid_search</span> <span class="o">=</span> <span class="nc">GridSearchCV</span><span class="p">(</span>
            <span class="n">estimator</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
            <span class="n">param_grid</span><span class="o">=</span><span class="n">param_grids</span><span class="p">[</span><span class="n">name</span><span class="p">],</span>
            <span class="n">return_train_score</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
            <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span>
            <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
        <span class="p">).</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_transformed</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">grid_search</span><span class="p">.</span><span class="n">best_params_</span><span class="p">,</span> <span class="n">grid_search</span><span class="p">.</span><span class="n">best_score_</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>============ One Hot Encoder
Random Forest {'n_estimators': 100} 0.7237796612387587
Hist Gradient Boosting {'max_iter': 50} 0.7663063586806613
ElasticNet {'alpha': 0.01, 'l1_ratio': 0.2} 0.689208549248405
xgb {'n_estimators': 20} 0.729716345667839
============ Ordinal Encoder
Random Forest {'n_estimators': 20} 0.7380497136926296
Hist Gradient Boosting {'max_iter': 50} 0.7970622263403604
ElasticNet {'alpha': 0.1, 'l1_ratio': 0.8} 0.6327096252754385
xgb {'n_estimators': 20} 0.7482473403215408
============ PCA
Random Forest {'n_estimators': 100} 0.5839788307210804
Hist Gradient Boosting {'max_iter': 50} 0.6267378200940041
ElasticNet {'alpha': 1, 'l1_ratio': 1.0} 0.6129175015796107
xgb {'n_estimators': 20} 0.6011636257171631
</code></pre></div></div> <p>From this comparison, we can see that the ensemble models perform better with ordinal encoding, while the linear model works better with one-hot encoding. Overall, the best-performing models are HistGradientBoosting and XGBoost.</p> <p>One thing I like to do is combine the best models using a voting ensemble. In my experience, this approach tends to be more beneficial for classification tasks, but it can also provide improvements for regression.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">reg1</span> <span class="o">=</span> <span class="nc">HistGradientBoostingRegressor</span><span class="p">(</span><span class="n">early_stopping</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">reg2</span> <span class="o">=</span> <span class="n">xgb</span><span class="p">.</span><span class="nc">XGBRegressor</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">v_reg</span> <span class="o">=</span> <span class="nc">VotingRegressor</span><span class="p">(</span><span class="n">estimators</span><span class="o">=</span><span class="p">[(</span><span class="sh">'</span><span class="s">hgb</span><span class="sh">'</span><span class="p">,</span> <span class="n">reg1</span><span class="p">),</span> <span class="p">(</span><span class="sh">'</span><span class="s">rf</span><span class="sh">'</span><span class="p">,</span> <span class="n">reg2</span><span class="p">)])</span>
<span class="nf">float</span><span class="p">(</span><span class="nf">cross_val_score</span><span class="p">(</span><span class="n">v_reg</span><span class="p">,</span> <span class="n">X_transformed_2</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="sh">'</span><span class="s">r2</span><span class="sh">'</span><span class="p">).</span><span class="nf">mean</span><span class="p">())</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0.7723058174761523
</code></pre></div></div> <p>This time, there was no noticeable improvement; therefore, we will proceed with the <code class="language-plaintext highlighter-rouge">HistGradientBoostingRegressor</code> moving forward.</p> <h3 id="model-training">Model Training</h3> <p>Now that we’ve selected the model, we can perform hyperparameter tuning to further improve its performance. We only test two values per parameter, as the number of tests conducted by the grid search increases exponentially with the number of parameters.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="nc">HistGradientBoostingRegressor</span><span class="p">(</span>
        <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">early_stopping</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">"</span><span class="s">learning_rate</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span>  <span class="c1"># Lower values for more stable learning
</span>    <span class="sh">"</span><span class="s">max_iter</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">],</span>  <span class="c1"># Number of boosting iterations
</span>    <span class="sh">"</span><span class="s">max_leaf_nodes</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span><span class="mi">15</span><span class="p">,</span> <span class="mi">31</span><span class="p">],</span>  <span class="c1"># Controls tree complexity
</span>    <span class="sh">"</span><span class="s">min_samples_leaf</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">],</span>  <span class="c1"># Minimum samples per leaf
</span>    <span class="sh">"</span><span class="s">l2_regularization</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span>  <span class="c1"># Regularization strength
</span>    <span class="sh">"</span><span class="s">max_depth</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span>  <span class="c1"># Depth of each tree (None means unlimited)
</span>    <span class="sh">"</span><span class="s">loss</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span><span class="sh">"</span><span class="s">squared_error</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">absolute_error</span><span class="sh">"</span><span class="p">],</span>  <span class="c1"># Loss function (MSE or MAE)
</span><span class="p">}</span>
<span class="n">cv</span> <span class="o">=</span> <span class="nc">KFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">grid_search</span> <span class="o">=</span> <span class="nc">GridSearchCV</span><span class="p">(</span>
            <span class="n">estimator</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
            <span class="n">param_grid</span><span class="o">=</span><span class="n">param_grid</span><span class="p">,</span>
            <span class="n">return_train_score</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
            <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span>
            <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
        <span class="p">).</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_transformed_2</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">grid_search</span><span class="p">.</span><span class="n">best_params_</span><span class="p">,</span> <span class="n">grid_search</span><span class="p">.</span><span class="n">best_score_</span><span class="p">)</span>

<span class="n">final_pipeline</span> <span class="o">=</span> <span class="n">preprocessing_pipeline_2</span>
<span class="n">final_model</span> <span class="o">=</span> <span class="nc">HistGradientBoostingRegressor</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">early_stopping</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="o">**</span><span class="n">grid_search</span><span class="p">.</span><span class="n">best_params_</span><span class="p">)</span>
<span class="n">final_model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_transformed_2</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</code></pre></div></div> <p>After obtaining the best parameters, we can train the model using all available training data and are now ready to evaluate its performance.</p> <h3 id="model-evaluation">Model Evaluation</h3> <p>Before using the testing data, we want to evaluate the model on the training data. We would expect good performance, as the model has seen all the data points, but we wouldn’t want perfect scores, as this would indicate overfitting. Additionally, we can plot the actual versus predicted counts to visually assess the model’s accuracy.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_predictions</span> <span class="o">=</span> <span class="n">final_model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_transformed_2</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">train_predictions</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">mae</span> <span class="o">=</span> <span class="nf">mean_absolute_error</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">train_predictions</span><span class="p">)</span>
<span class="n">mse</span> <span class="o">=</span> <span class="nf">mean_squared_error</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">train_predictions</span><span class="p">)</span>
<span class="n">r2</span> <span class="o">=</span> <span class="nf">r2_score</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">train_predictions</span><span class="p">)</span>
<span class="n">mape</span> <span class="o">=</span> <span class="nf">mean_absolute_percentage_error</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">train_predictions</span><span class="p">)</span>
<span class="n">explained_variance</span> <span class="o">=</span> <span class="nf">explained_variance_score</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">train_predictions</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Mean Absolute Error (MAE): </span><span class="si">{</span><span class="n">mae</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Mean Squared Error (MSE): </span><span class="si">{</span><span class="n">mse</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">R-squared (R²): </span><span class="si">{</span><span class="n">r2</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Mean Absolute Percentage Error (MAPE): </span><span class="si">{</span><span class="n">mape</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Explained Variance Score: </span><span class="si">{</span><span class="n">explained_variance</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Mean Absolute Error (MAE): 23.31
Mean Squared Error (MSE): 1688.57
R-squared (R²): 0.93
Mean Absolute Percentage Error (MAPE): 0.32
Explained Variance Score: 0.93
</code></pre></div></div> <p style="text-align:center"><img src="../../../assets/img/blog_images/2025-02-24-weather-data-to-bike-sharing-demand/output_46_1.png" alt="Correlation matrix" style="max-width: 100%; height: auto;"></p> <p>Another way to visualise the predictions versus the actual data is by comparing each data point for each instant.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="sh">"</span><span class="s">instant</span><span class="sh">"</span><span class="p">],</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="sh">"</span><span class="s">instant</span><span class="sh">"</span><span class="p">],</span> <span class="n">train_predictions</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">xticks</span><span class="p">(</span><span class="n">rotation</span><span class="o">=</span><span class="mi">45</span><span class="p">)</span>
</code></pre></div></div> <p style="text-align:center"><img src="../../../assets/img/blog_images/2025-02-24-weather-data-to-bike-sharing-demand/output_48_0.png" alt="Correlation matrix" style="max-width: 100%; height: auto;"></p> <p>From the last plot, we can see that the model tends to underestimate values towards the end, predicting lower than the actual values. This may suggest that the number of users is increasing over time, and the model is not capturing this trend effectively, even in the training data.</p> <p>Next, we evaluate the model using the test data. As expected, the plot and metrics show that the predictions are not as accurate as on the training data, but the results are still reasonably good.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">X_test_transform</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span>
<span class="n">X_test_transform</span><span class="p">[</span><span class="sh">'</span><span class="s">cnt_prev_day</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_test</span><span class="p">.</span><span class="nf">shift</span><span class="p">(</span><span class="mi">24</span><span class="p">)</span>
<span class="n">X_test_transform</span><span class="p">[</span><span class="sh">'</span><span class="s">cnt_prev_day</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">X_test_transform</span><span class="p">[</span><span class="sh">'</span><span class="s">cnt_prev_day</span><span class="sh">'</span><span class="p">].</span><span class="nf">fillna</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">X_test_transform</span> <span class="o">=</span> <span class="n">final_pipeline</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">X_test_transform</span><span class="p">)</span>

<span class="n">predictions</span> <span class="o">=</span> <span class="n">final_model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test_transform</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>

<span class="n">mae</span> <span class="o">=</span> <span class="nf">mean_absolute_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
<span class="n">mse</span> <span class="o">=</span> <span class="nf">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
<span class="n">r2</span> <span class="o">=</span> <span class="nf">r2_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
<span class="n">mape</span> <span class="o">=</span> <span class="nf">mean_absolute_percentage_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
<span class="n">explained_variance</span> <span class="o">=</span> <span class="nf">explained_variance_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Mean Absolute Error (MAE): </span><span class="si">{</span><span class="n">mae</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Mean Squared Error (MSE): </span><span class="si">{</span><span class="n">mse</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">R-squared (R²): </span><span class="si">{</span><span class="n">r2</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Mean Absolute Percentage Error (MAPE): </span><span class="si">{</span><span class="n">mape</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Explained Variance Score: </span><span class="si">{</span><span class="n">explained_variance</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Mean Absolute Error (MAE): 51.24
Mean Squared Error (MSE): 6882.47
R-squared (R²): 0.86
Mean Absolute Percentage Error (MAPE): 0.41
Explained Variance Score: 0.87
</code></pre></div></div> <p style="text-align:center"><img src="../../../assets/img/blog_images/2025-02-24-weather-data-to-bike-sharing-demand/output_53_1.png" alt="Correlation matrix" style="max-width: 100%; height: auto;"></p> <p>When plotting over time, we can observe what we discussed earlier: the model is underestimating values, likely because the number of users has increased over time. Since the model didn’t have access to future data, it couldn’t account for this trend.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="sh">"</span><span class="s">instant</span><span class="sh">"</span><span class="p">],</span> <span class="n">y_test</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="sh">"</span><span class="s">instant</span><span class="sh">"</span><span class="p">],</span> <span class="n">predictions</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">xticks</span><span class="p">(</span><span class="n">rotation</span><span class="o">=</span><span class="mi">45</span><span class="p">)</span>
</code></pre></div></div> <h3 id="model-interpretation">Model Interpretation</h3> <p>In this code, we use the permutation importance method to assess the significance of each feature in the trained model. The <code class="language-plaintext highlighter-rouge">permutation_importance</code> function computes the importance of each feature by randomly shuffling its values and evaluating the change in model performance. The output shows the mean importance scores and their associated standard deviations for each feature. Features with a significant impact on the model, such as <code class="language-plaintext highlighter-rouge">hr</code> and <code class="language-plaintext highlighter-rouge">cnt_prev_day</code>, are highlighted, while those with little effect, like <code class="language-plaintext highlighter-rouge">holiday</code>, have near-zero importance. This helps us understand which features are driving the model’s predictions.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">r</span> <span class="o">=</span> <span class="nf">permutation_importance</span><span class="p">(</span><span class="n">final_model</span><span class="p">,</span> <span class="n">X_transformed_2</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span>
                           <span class="n">n_repeats</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span>
                           <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>


<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">r</span><span class="p">.</span><span class="n">importances_mean</span><span class="p">.</span><span class="nf">argsort</span><span class="p">()[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
    <span class="k">if</span> <span class="n">r</span><span class="p">.</span><span class="n">importances_mean</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">r</span><span class="p">.</span><span class="n">importances_std</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">X_transformed_2</span><span class="p">.</span><span class="n">columns</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">:</span><span class="o">&lt;</span><span class="mi">8</span><span class="si">}</span><span class="se">\t</span><span class="sh">"</span>
              <span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">r</span><span class="p">.</span><span class="n">importances_mean</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span>
              <span class="sa">f</span><span class="sh">"</span><span class="s"> +/- </span><span class="si">{</span><span class="n">r</span><span class="p">.</span><span class="n">importances_std</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>hr      	    0.491 +/- 0.007
cnt_prev_day	    0.429 +/- 0.006
workingday	    0.091 +/- 0.002
weekday 	    0.074 +/- 0.002
instant 	    0.071 +/- 0.002
weathersit	    0.038 +/- 0.001
hum     	    0.033 +/- 0.001
temp    	    0.030 +/- 0.001
atemp   	    0.015 +/- 0.001
windspeed	    0.002 +/- 0.000
season  	    0.002 +/- 0.000
mnth    	    0.001 +/- 0.000
holiday 	    0.000 +/- 0.000
</code></pre></div></div> <h3 id="other-things-to-consider">Other things to consider</h3> <p><strong>Error Analysis</strong><br> It’s essential to investigate where the model performed poorly in order to identify areas for improvement. By focusing on these weak spots, we can refine the model and enhance its overall performance.</p> <p><strong>Consider the Increasing Trend of Users Over Time</strong><br> The increasing trend in user numbers over time may not be captured by the model, leading to underestimations, especially in later periods. Recognising this pattern can help in adjusting the model to better account for time-dependent factors.</p> <p><strong>Outliers</strong><br> Outliers can significantly affect the model’s predictions. It’s important to identify and understand them, as they may reveal underlying patterns or issues in the data that need addressing.</p> <p><strong>Reiterate</strong><br> After evaluating the model, make necessary adjustments to features, model parameters, or even consider trying a different model type. However, always be cautious not to overfit to the testing data by performing too many iterations. A separate validation set, which should only be used once, can help prevent overfitting.</p> <p><strong>CI/CD Stage</strong><br> In the CI/CD stage, the focus shifts to deploying the model and ensuring it is continually monitored for performance. Monitoring is key to maintaining model accuracy and catching any potential issues that may arise over time.</p> <h3 id="conclusions">Conclusions</h3> <p>In this blog post, we explored the process of building a regression model to predict bike-sharing demand. Through data preprocessing, feature engineering, and model selection, we developed a robust model that provides valuable insights. While the model performs well overall, there are areas for improvement, such as accounting for the increasing trend of users over time.</p> <p>I hope this blog post has been useful to you in understanding the process of building and evaluating a regression model, and that you can apply these techniques to your own projects. If you have any feedback or suggestions for improvement, feel free to reach out via <a href="mailto:mario_hevia@hotmail.com">email</a> or <a href="https://www.linkedin.com/in/mario-hevia/" rel="external nofollow noopener" target="_blank">LinkedIn</a>.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2026 Mario A. Hevia Fajardo. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: February 03, 2026. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>